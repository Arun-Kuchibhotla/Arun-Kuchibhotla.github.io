\documentclass[10pt]{article}
\usepackage[compact,small]{titlesec}
% \def\baselinestretch{.965}

\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
% \usepackage{fullpage}
\usepackage[square, numbers]{natbib}
% \usepackage{natbib}
\setlength{\bibsep}{0.5pt}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\usepackage[font=itshape]{quoting}
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information
% \usepackage{setspace}
% \doublespacing
\usepackage[ruled,linesnumbered, vlined, noend]{algorithm2e}         
\usepackage{amsthm, amsfonts, amssymb, amsmath,enumitem, bbm, mathabx}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{prop}{Proposition}
\newtheorem{conjecture}{Conjecture}
\newcounter{myalgctr}
\newenvironment{rem}{%      define a custom environment
   \vskip1mm\indent%         create a vertical offset to previous material
   \refstepcounter{myalgctr}% increment the environment's counter
   \textbf{Remark \themyalgctr}% or \textbf, \textit, ...
   }{\hfill$\diamond$\par}  %          create a vertical offset to following material
\numberwithin{myalgctr}{section}
% \providecommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareRobustCommand{\rchi}{{\mathpalette\irchi\relax}}
\newcommand{\irchi}[2]{\raisebox{\depth}{$#1\chi$}}
\usepackage{graphicx} % support the \includegraphics command and options
\usepackage{color, float}
% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
% \usepackage{booktabs} % for much better looking tables
% \usepackage{array} % for better arrays (eg matrices) in maths
%\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
% \usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
% \usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage[colorlinks=true, a4paper=true, pdfstartview=FitV,
linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage{authblk}
%%% HEADERS & FOOTERS
% \usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
% \pagestyle{fancy} % options: empty , plain , fancy
% \renewcommand{\headrulewidth}{0pt} % customise the layout...
% \lhead{}\chead{}\rhead{}
% \lfoot{}\cfoot{\thepage}\rfoot{}
%% The following for description environment referencing
\makeatletter
\def\namedlabel#1#2{\begingroup
    #2%
    \def\@currentlabel{#2}%
    \phantomsection\label{#1}\endgroup
}
\makeatother
% \newcommand{\vertiii}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
%     \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}
% %%% SECTION TITLE APPEARANCE
% \usepackage{sectsty}
% \allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% % (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
% \usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
% \usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
% \renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
% \renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!
\newcommand{\RIP}{\mbox{RIP}}
\newcommand{\M}{\mathrm{M}}
%%% END Article customizations
% \usepackage{geometry}
\title{Uncertainty Quantification using Conformal Prediction: Hyper-parameter Tuning and Over-parametrization}
\author{Arun Kumar Kuchibhotla}
\affil{5217 Fifth Ave, Pittsburgh, PA -- 15232\\Email: \texttt{arunku@stat.cmu.edu}, Tel: 267 693 3354.\\Department of Statistics and Data Science\\ Carnegie Mellon University, Pittsburgh.}
\date{}

\begin{document}

% \maketitle
% % \vspace{-.3in}
% % \begin{abstract}
% \section{Abstract}
% Conformal prediction~\cite{vovk2005algorithmic} is a generic tool to provide uncertainty quantification for black-box prediction algorithms. 
% Understanding uncertainty is important for applications such as medicine and self-driving cars where deep learning models are becoming more common. 
% Because conformal prediction is agnostic to the algorithm, there remain questions of tuning the hyper-parameters in machine learning algorithms to obtain the smallest prediction set while retaining  prediction validity. In this proposal, we plan to tackle this problem by providing a method of hyper-parameter tuning for predictive inference and proving that the method chooses a near optimal prediction set. Further, we plan to investigate the behavior of conformal prediction sets under over-parameterization, where a double-descent phenomenon~\cite{belkin2019reconciling} is known to occur. 
% % \end{abstract}
% % Recent success of deep learning and neural networks has brought to limelight the ``double descent phenomenon''~\citep{belkin2019reconciling}: generalization performance first improves, then gets worse, and then improves again with increasing model size, data size, or training time. Several researchers in recent years have contributed to the understanding of this phenomenon by providing theoretical arguments in specific models or data generating processes. Implications of such double descent for uncertainty quantification has not received much attention. Our research agenda is to use conformal prediction methodology to provide sound uncertainty quantification and adapt to double descent phenomenon in training, whenever possible.
% \section{Research Goals}
% Conformal prediction is a distribution-free finite-sample valid uncertainty quantification for prediction. This was introduced by~\cite{vovk2005algorithmic} and has become widely popular in statistics and machine learning~\cite{lei2014distribution,angelopoulos2020uncertainty,barber2019predictive} because it can act as a wrapper around any algorithm that provides point predictions. As a quick illustration, the inductive~\citep{papadopoulos2002inductive} or split~\citep{lei2014distribution} conformal prediction method works as follows: Suppose we have independent and identically distributed (i.i.d.) training data $(X_1, Y_1), \ldots, (X_n, Y_n)$ and i.i.d. calibration data $(X_1', Y_1'), \ldots, (X_m', Y_m')$ from a distribution $P$ on some space $\mathcal{X}\times\mathbb{R}$. The space $\mathcal{X}$ can be arbitrary such as a function space, a space of images, etc. Let $\mathcal{A}:\mathcal{X}\to\mathbb{R}$ be any predictive algorithm trained on the training data, i.e., for any $x\in\mathcal{X}$, $\mathcal{A}(x)$ is the point prediction for $Y$. If $R_i' = |Y_i' - \mathcal{A}(X_i')|, 1\le i\le m$ are residuals for the calibration data, and $\widehat{Q}_{\alpha}$ is the $\lceil(m+1)(1-\alpha)\rceil$-th largest element of $\{R_i', 1\le i\le m\}$, then for any $(X_{\mathrm{f}}, Y_{\mathrm{f}})\sim P$, $\mathbb{P}(|Y_{\mathrm{f}} - \mathcal{A}(X_{\mathrm{f}})| \le \widehat{Q}_{\alpha}) \ge 1 - \alpha.$
% In particular, for $\widehat{C}(x) = \{y\in\mathbb{R}:\,|y - \mathcal{A}(x)| \le \widehat{Q}_{\alpha}\}$, we have $\mathbb{P}(Y_{\mathrm{f}} \in \widehat{C}(X_{\mathrm{f}})) \ge 1 - \alpha$. 

% The prediction guarantee above is agnostic to the training algorithm $\mathcal{A}$ and the distribution $P$ of the training/calibration data. With a finite-sample valid prediction set, conformal prediction provides us with an  understanding of the sampling uncertainty in the algorithm and the data. There are several other versions of conformal prediction that make better use of the training/calibration data~\citep{barber2019predictive,gupta2019nested} and also apply to classification settings~\cite{angelopoulos2020uncertainty,gupta2019nested}. 

% In spite of these nice features of conformal prediction, questions remain in light of over-parametrization and hyper-parameter tuning. For example, conformal methodology can be applied to neural networks with arbitrary architectures and still have validity guarantees. The question remains on how to construct architectures with smallest prediction sets that also have validity guarantees. 
% % One naive method would be to construct several algorithms (with different architectures) $\mathcal{A}_1, \ldots, \mathcal{A}_K$ on the training data and select the one that yields the smallest prediction set, in terms of width. Although, this leads to smallest prediction set, it does not retain validity. 
% Therefore, the first problem we plan to tackle is:
% \begin{quoting}
% \textbf{(P1)}\;Given a collection of training algorithms, construct a selection rule that yields a near optimal prediction set and yet retains finite-sample validity. Further, how close can one get to the smallest prediction set in terms of width?
% \end{quoting}
% This problem was mentioned in~\cite{lei2014distribution}. Cross-validation (CV) might be the first choice, but CV targets the prediction risk instead of the width of a prediction set. Algorithm~1 of~\cite{lei2014distribution} (repeated here as Algorithm~\ref{alg:selection-rule}) provides a viable way to pick the smallest prediction set, but a formal study is left open.  
% \begin{algorithm}[t]
%     \SetAlgoLined
%     \SetEndCharOfAlgoLine{}
%     \KwIn{Data $(X_i, Y_i), 1\le i\le n$, coverage probability $1 - \alpha$, and algorithms $\mathcal{A}_k, 1\le k\le K$.}
%     \KwOut{A valid prediction set $\widehat{C}^{\mathrm{final}}(\cdot)$ with width close to smallest width.}
%     % \textbf{Initialization:} Data $(X_i, Y_i), 1\le i\le n$\;
%     % \hspace{0.1in} 
%     Split the data $\mathcal{D}$ into three parts $\mathcal{D}_1, \mathcal{D}_2,$ and $\mathcal{D}_3$. Fit algorithms $\mathcal{A}_1, \ldots, \mathcal{A}_K$ on $\mathcal{D}_1$.\;
%     % \hspace{0.1in} 
%     Calculate the ``residuals'' on data $\mathcal{D}_2$ for each algorithm $\mathcal{A}_k, 1\le k\le K$ and compute the corresponding conformal prediction sets $\widehat{C}_1, \widehat{C}_2, \ldots, \widehat{C}_K$ with confidence level $1 - \alpha$.\;
%     % \hspace{0.1in} 
%     Set $\widehat{k} := \argmin_{1\le k\le K}\,\mbox{Width}(\widehat{C}_k)$ and calculate the ``residuals'' on $\mathcal{D}_3$ for algorithm $\mathcal{A}_{\widehat{k}}$.\;
%     Report the conformal prediction set $\widehat{C}^{\mathrm{final}}(\cdot)$ obtained from $\mathcal{D}_3$ and $\mathcal{A}_{\widehat{k}}$.\;
%     \Return $\widehat{C}^{\mathrm{final}}(\cdot)$.
%     \caption{Selection Rule for Smallest Prediction Set}
%     \label{alg:selection-rule}
% \end{algorithm}
% % Suppose we have data $\mathcal{D} = \{(X_i, Y_i), 1\le i\le n\}$.
% % \begin{enumerate}
% %     \item Split the data $\mathcal{D}$ into three parts $\mathcal{D}_1, \mathcal{D}_2,$ and $\mathcal{D}_3$. Fit algorithms $\mathcal{A}_1, \ldots, \mathcal{A}_K$ on $\mathcal{D}_1$.
% %     \item Calculate the ``residuals'' on data $\mathcal{D}_2$ for each algorithm $\mathcal{A}_k, 1\le k\le K$ and compute the corresponding conformal prediction sets $\widehat{C}_1, \widehat{C}_2, \ldots, \widehat{C}_K$ with confidence level $1 - \alpha$.
% %     \item Set $\widehat{k} := \argmin_{1\le k\le K}\,\mbox{Width}(\widehat{C}_k)$ and calculate the ``residuals'' on $\mathcal{D}_3$ for algorithm $\mathcal{A}_{\widehat{k}}$.
% %     \item Report the conformal prediction set $\widehat{C}^{\mathrm{final}}(\cdot)$ obtained from $\mathcal{D}_3$ and $\mathcal{A}_{\widehat{k}}$.
% % \end{enumerate}

% It can be proved that the outputted prediction set $\widehat{C}^{\mathrm{final}}$ based on Algorithm~\ref{alg:selection-rule} has a guaranteed $1 - \alpha$ coverage probability in finite samples. Observe that
% although the prediction set $\widehat{C}_{\widehat{k}}$ computed in steps 2 \& 3 has the smallest width, it does not guarantee validity. This is similar to double dipping or cherry-picking. If $C^*_k, 1\le k\le K$ represent ``oracle'' versions of the prediction sets $\widehat{C}_k, 1\le k\le K$ (in step 2), then the aim of~\textbf{(P1)} is to prove that for some $R_{n,K}$ converging to zero with the sample size $n$ (and $|\mathcal{D}_3|$), with high probability, 
% \begin{equation}\label{eq:oracle-inequality}%\textstyle
% \mathrm{Width}(\widehat{C}^{\mathrm{final}}) ~\le~ \min_{1\le k\le K}\mathrm{Width}(C^*_k) + R_{n,K}.
% \end{equation}
% This is a form of oracle inequality. Understanding the dependence of $R_{n,K}$ on $K$, the number of algorithms, is crucial. Here $K$ can be infinite if the class of algorithms is small (in VC-dimension or metric entropy etc). Problem~\textbf{(P1)} is similar to aggregation in machine learning, where we want to pick one or a combination from a given collection of estimators that leads to a near optimal risk~\citep{bunea2007aggregation}.%~\citep{tsybakov2003optimal}. 
% \medskip

% A related second problem we plan to study stems from the empirical observation that if an algorithm undergoes a double descent~\cite{belkin2019reconciling}, then the corresponding conformal prediction width also undergoes a similar double descent; see Figure~\ref{fig:double-descent}. %Double descent phenomenon~\cite{belkin2019reconciling} refers to the observation that generalization performance first improves, then gets worse, and then improves again with increasing model size, data size, or training time.
% \begin{figure}[!h]
%     \centering
%     \includegraphics[width = \textwidth]{Ramdas_figure2.jpg}
%     \caption{Taken from~\citet[Fig. 2]{barber2019predictive}. This shows the width of conformal prediction intervals obtained from minimum norm linear regression estimators as the dimension $d$ increases and the sample size $n$ is fixed at 100. The methods `naive' and `jackknife' do not have coverage guarantees. The split method uses only half the data for calibrating, causing it to peak, in width, around $d = n/2 = 50$.}
%     \label{fig:double-descent}
% \end{figure}
% It is of interest to investigate the double-descent phenomenon in conformal prediction and understand adaptation to such a phenomenon if it holds for the data at hand. 
% The second problem we plan to tackle is, therefore:
% \begin{quoting}
% \textbf{(P2)} Prove that conformal prediction methods also exhibit a double descent phenomenon. Further, if we have a class of training algorithms that undergo double descent in one or more of their hyper-parameters (model size, width of network, etc), then construct a method to tune the hyper-parameters to take advantage of double descent in obtaining a smaller prediction set.
% \end{quoting}
% The first part of~\textbf{(P2)} would most probably follow the same techniques as used in the proofs for generalization error. But there is a difference in that we now deal with quantiles instead of prediction errors, and we believe it is of importance to theoretically understand the implications for uncertainty quantification. The second part of~\textbf{(P2)} could follow from \textbf{(P1)}, but this requires analyzing the minimum of $\mathrm{Width}(C_k^*), 1\le k\le K$ to understand whether the minimum width is attained before/after the critical threshold. 
% \section{Expected Outcomes}
% We expect to prove an oracle inequality of the type~\eqref{eq:oracle-inequality} for a large class of conformal prediction methods in the literature including the split conformal~\cite{papadopoulos2002inductive,lei2014distribution}, jackknife+~\cite{barber2019predictive}, and nested conformal~\cite{gupta2019nested} methods. In ongoing work, we have succeeded in proving~\eqref{eq:oracle-inequality} when the algorithms $\mathcal{A}_k$ in Algorithm~\ref{alg:selection-rule} are ridge regressions $\widehat{\beta}_{\lambda}, \lambda\ge -\Lambda_{\min}/2$, where $\Lambda_{\min}$ is the minimum eigenvalue of $\mathbb{E}[XX^{\top}]$. Formally, we prove~\eqref{eq:oracle-inequality} with $R_{n,K} = R_{n,\infty} = \sqrt{d/n}$, where $d$ is the dimension of the feature vector $X$. The case where $d/n\to\gamma\in(0,\infty)$ is under investigation.%; this is where we expect to also prove a double descent phenomenon.

% Regarding problem~\textbf{(P2)}, we expect to prove the double descent phenomenon observed in Figure~\ref{fig:double-descent} for linear regression,
% % as the number of features increases
% and then extend the proofs to random forests and neural networks~\cite[Figs. 3 \& 4]{belkin2019reconciling}.
% \section{Related Prior Work}%\label{sec:prior-work}
% % \paragraph{Prior Work.}
% Conformal prediction is currently a very active research area in statistics and machine learning~\cite{papadopoulos2002inductive,lei2014distribution,lei2018distribution,barber2019predictive,gupta2019nested,angelopoulos2020uncertainty}. Most papers concentrate on developing new ``residual'' measures~\cite{gupta2019nested,romano2019conformalized} or on making better use of the training data~\cite{barber2019predictive,gupta2019nested} for a fixed algorithm. In practice, however, most algorithms have tuning parameters, and the choice is important for precise prediction (see~\cite{lei2014distribution,lei2018distribution}). The proof of oracle inequality~\eqref{eq:oracle-inequality} is left as an open question in~\cite{lei2014distribution}.
% To our knowledge, uncertainty quantification via conformal prediction under over-parametrization has not yet been addressed in the literature. Although the double descent phenomenon was observed in~\cite{barber2019predictive}, it hasn't been investigated theoretically.
% % \section{Data Policy}
% % The proposed work has both theoretical and practical/methodological implications. 
% \paragraph{Data Policy.}
% The theoretical part of the work will be made freely available through \url{arxiv.org}. The methodological part of the work will be implemented in Python and R and made accessible through Github.
% \bibliographystyle{apalike}
% \bibliography{AssumpLean}
% % \hrulefill\\
% \newpage
% \makeheading{Arun Kumar Kuchibhotla}
{\large\bf\noindent Arun Kumar Kuchibhotla}\\
\rule{\textwidth}{1pt}
% \section*{Contact Information}
\paragraph{Contact Information}
\begin{itemize}
\item[]
\newlength{\rcollength}\setlength{\rcollength}{1.8in}%
%
\begin{tabular}[t]{@{}p{\textwidth-\rcollength}p{\rcollength}}
\href{http://www.stat.cmu.edu/}%
    {Department of Statistics and Data Science} & \\
\href{https://www.cmu.edu/}{Carnegie Mellon University} &\\
Email: {arunku@cmu.edu} & \\
Website: \url{https://arun-kuchibhotla.github.io/} &
\end{tabular}
\end{itemize}
\paragraph{Research Interests}
% \section*{Research Interests}
\begin{itemize}
    \item[] Post-selection Inference, Large Sample Theory, Conformal Prediction, Concentration Inequalities.
\end{itemize}
\paragraph{Academic Positions}
% \section*{Academic Positions}
\begin{itemize}
    \item[] Assistant Professor, Department of Statistics \& Data Science, Carnegie Mellon University \\ 2020 --- current.
\end{itemize}
% \paragraph{Education}
\section*{Education}
\begin{itemize}\itemsep0em
    \item University of Pennsylvania: Ph.D. in Statistics \hfill 2015--2020\\  \hspace*{1.02cm}\textit{Thesis Advisors}: (Late) Lawrence D. Brown, Andreas Buja.
    \item University of Pennsylvania: Master of Arts in Statistics \hfill 2015--2016
    \item Indian Statistical Institute: Master of Statistics (Distinction) \hfill 2013--2015\\
    \hspace*{1.3cm}\textit{Specialization}: Mathematical Statistics and Probability
    \item Indian Statistical Institute: Bachelor of Statistics (Distinction) \hfill 2010--2013
\end{itemize}
\vspace{-0.1in}

% \paragraph{Journal Publications}
\section*{Journal Publications}
% \setlength{\leftmargini}{1em}
\begin{enumerate}\itemsep0em
\item \textbf{Kuchibhotla A. K.} and Basu A. (2015) ``A General Set Up for Minimum Disparity Estimation." \emph{Statistics and Probability Letters},  Vol. 96, 68-74.
\item \textbf{Kuchibhotla A. K.} and Basu A. (2017) ``On The Asymptotics of Minimum Disparity Estimation.'' \href{https://www.springer.com/journal/11749}{\emph{TEST}}, 26 (3):481--502.  
\item \textbf{Kuchibhotla A. K.}, Mukherjee S. and Basu A. (2017) ``Statistical inference based on bridge divergences.'' \emph{Annals of the Institute of Statistical Mathematics, 71 (3), 627-656.}
\item Berk R., Buja A., Brown L. D., George E. I., \textbf{Kuchibhotla A. K.}, Su W. J., Zhao L. H., (2019) ``Assumption Lean Regression'' \emph{The American Statistician, 1-17.}
\item Buja A., Brown L. D., \textbf{Kuchibhotla A. K.}, Berk R., George E. I., Zhao L. H. (2019) ``Models as Approximations -- Part II: A General Theory of Model-Robust Regression'' \emph{Statistical Science, 34(4), 545--565.}
\item Bellec P., and \textbf{Kuchibhotla A. K.}, (2019) ``First order expansion of convex regularized estimators'' \emph{Advances in Neural Information Processing Systems 32. Pages 3462--3473.}
\item \textbf{Kuchibhotla A. K.}, Brown L. D., Buja A., Cai, J., George E.I., Zhao L.H. (2019) ``Valid Post-selection Inference in Model-free Linear Regression." \emph{Annals of Statistics}, 48(5), 2953--2981.
\item \textbf{Kuchibhotla A. K.} and Parta R. K. (2020) ``Efficient Estimation in Single Index Models through Smoothing splines'' \emph{Bernoulli}, 26(2), 1587--1618.
\item \textbf{Kuchibhotla A. K.}, Banerjee D., Muhkerjee S. (2021) ``High-dimensional CLT: Improvements, Non-uniform Extensions and Large Deviations'' \emph{Bernoulli}, 27(1): 192-217.
	\item \textbf{Kuchibhotla A. K.} and Zheng Q. (2021) ``Near-Optimal Confidence Sequences for Bounded Random Variables'' \href{https://arxiv.org/abs/2006.05022}{\emph{arXiv:2006.05022.}} \emph{Proceedings of the 38th International Conference on Machine Learning, PMLR} 139:5827-5837.
\item \textbf{Kuchibhotla A. K.}, Patra R. K., Sen B. (2021) ``Semiparametric Efficiency in Convexity Constrained Single Index Model'' \href{https://arxiv.org/abs/1708.00145}{\emph{arXiv:1708.00145.}} Accepted at \emph{Journal of American Statistical Association}.
\item \textbf{Kuchibhotla A. K.}, Brown L. D., Buja A., George E.I., Zhao L.H. (2021) ``A Model Free Perspective for Linear Regression: Uniform-in-model Bounds for Post Selection Inference'' \href{https://arxiv.org/abs/1802.05801}{\emph{arXiv:1802.05801.}} Accepted at \emph{Econometric Theory}.
	\item Gupta C., \textbf{Kuchibhotla A. K.}, and Ramdas. A. (2021) ``Nested conformal prediction and quantile out-of-bag ensemble methods'' \href{https://arxiv.org/abs/1910.10562}{\emph{arXiv:1910.10562.}} Accepted at \emph{Pattern Recognition.}
	\item \textbf{Kuchibhotla A. K.} and Patra R. K. (2021) ``On Least Squares Estimation under Heteroscedastic and Heavy-Tailed Errors'' \href{https://arxiv.org/abs/1909.02088}{\emph{arXiv:1909.02088.}} Accepted at \emph{Annals of Statistics}.
	\item \textbf{Kuchibhotla A. K.}, Kolassa J. E., and Kuffner T. A. (2022) ``Post-selection Inference''  \emph{Annual Review of Statistics and Its Application}, Volume 9.
	\item Ogburn E. L., Cai J., \textbf{Kuchibhotla A. K.}, Berk R. A., Buja A. (2022) ``A few practical issues concerning assumption-lean inference for generalized linear models'' [Comment on ``Assumption-lean inference for generalised linear model
parameters''] Accepted at \emph{Journal of the Royal Statistical Society: Series B (Statistical Methodology)}.
    % \item 
\end{enumerate}
% \paragraph{Preprints}
\section*{Preprints}
\begin{enumerate}\itemsep0em
	\item \textbf{Kuchibhotla A. K.}, Chakrabortty A. (2018) ``Moving Beyond Sub-Gaussianity in High-Dimensional Statistics: Applications in Covariance Estimation and Linear Regression'' \emph{arXiv:1804.02605.} Minor revision at Information and Inference: a Journal of the IMA.
	\item Chakrabortty A. and \textbf{Kuchibhotla A. K.} (2018) ``Tail Bounds for Canonical U-Statistics and U-Processes with Unbounded Kernels''
	\item \textbf{Kuchibhotla A. K.}, Brown L. D., Buja A. (2018) ``Model-free Study of Ordinary Least Squares Linear Regression'' \href{https://arxiv.org/abs/1809.10538}{\emph{arxiv:1809.10538.}}
	\item \textbf{Kuchibhotla A. K.} (2018) ``Deterministic Inequalities for Smooth M-estimators'' \href{https://arxiv.org/abs/1809.05172}{\emph{arxiv:1809.05172.}} 
  \item \textbf{Kuchibhotla A. K.} (2020) ``Exchangeability, Conformal Prediction, and Rank Tests'' \href{https://arxiv.org/abs/2005.06095}{\emph{arXiv:2005.06095.}} Revision submitted to Statistical Science.
  \item \textbf{Kuchibhotla A. K.}, Rinaldo A., and Wasserman L. (2020) ``Berry-Esseen Bounds for Projection Parameters and Partial Correlations with Increasing Dimension'' \href{https://arxiv.org/abs/2007.09751}{\emph{arXiv:2007.09751.}} Revision submitted to the Annals of Statistics.
  \item \textbf{Kuchibhotla A. K.} and Rinaldo, A. (2020) ``High-dimensional CLT for Sums of Non-degenerate Random Vectors: $n^{-1/2}$-rate'' \href{https://arxiv.org/abs/2009.13673}{\emph{arXiv:2009.13673.}}
  \item \textbf{Kuchibhotla A. K.} and Berk R. A. ``Nested Conformal Prediction Sets for Classification with Applications to Probation Data'' \href{https://arxiv.org/abs/2104.09358}{\emph{arXiv:2104.09358.}} Minor revision at the Annals of Applied Statistics.
  \item Yang Y., and \textbf{Kuchibhotla A. K.} (2021) ``Finite-sample Efficient Conformal Prediction'' \href{https://arxiv.org/abs/2104.13871}{\emph{arXiv:2104.13871.}} Submitted to the Journal of the American Statistical Association.
  \item \textbf{Kuchibhotla A. K.}, Balakrishnan, S., and Wasserman L. (2021) ``The HulC: Confidence Regions from Convex Hulls'' \href{https://arxiv.org/abs/2105.14577}{\emph{arXiv:2105.14577.}} Submitted to the Journal of Royal Statistical Society: Series B (Statistical Methodology).
  \item \textbf{Kuchibhotla A. K.} (2021) ``Median bias of M-estimators'' \href{https://arxiv.org/abs/2106.00164}{\emph{arXiv:2106.00164.}}
\item Berk R., \textbf{Kuchibhotla A. K.}, and Tchetgen Tchetgen E. J. (2021) ``Improving Fairness in Criminal Justice Algorithmic Risk Assessments Using Optimal Transport and Conformal Prediction Sets'' \href{https://arxiv.org/pdf/2111.09211.pdf}{\emph{arXiv:2111.09211.}}
  \item Fogliato R., Shrotriya S., and \textbf{Kuchibhotla A. K.} ``maars: Tidy Inference under the 'Models as Approximations' Framework in R'' \href{https://arxiv.org/abs/2106.11188}{\emph{arXiv:2106.11188.}}
  \item Yang Y., \textbf{Kuchibhotla A. K.}, and Tchetgen Tchetgen E. J. (2022) ``Doubly Robust Calibration of Prediction Sets under Covariate Shift.'' \href{https://arxiv.org/abs/2203.01761}{\emph{arxiv:2203.01761.}}
\end{enumerate}
\section*{Working Papers}
\begin{enumerate}
    \item \textbf{Kuchibhotla A. K.}, Balakrishnan, S., and Wasserman L. (2022) ``A new notion of regularity of an estimator and its equivalence with valid inference.''
    \item Yang Y., \textbf{Kuchibhotla A. K.}, and Tchetgen Tchetgen E. J. (2022) ``Minimax optimal non-parametric regression for missing data and CATE estimation.''
    \item Hong A., and \textbf{Kuchibhotla A. K.} (2022) ``Statistical Practice: Examples and Issues of Reproducibility.''
    \item \textbf{Kuchibhotla A. K.}, and Mukherjee S. S. (2022) ``Sharp Maximal Inequalities.''
\end{enumerate}
% \section*{Teaching}
\paragraph{Teaching}
\begin{itemize}\itemsep0em
\item 46-929, CMU -- Financial Time Series Analysis (Spring 2022)
\item 36-761, CMU -- Modern Theory of Linear Regression (Fall 2021)
\item 36-760, CMU -- Central Limit Theorems and Resampling (Fall 2021)
\item 37-761, CMU -- Modern Theory of Linear Regression (Fall 2020)
\item 36-760, CMU -- Concentration Inequalities and CLTs (Fall 2020)
\item STAT991, UPenn -- Topics in Linear Models (Fall 2017)
% \\
% ---~ Taught half a course jointly with Lawrence D. Brown. Part of creating the course structure and lecture notes.
\item STAT111, UPenn -- Introductory Statistics (Summer 2018)
% \\
% ---~ Created syllabus, course material and lectured five times a week to a class of ∼25 undergraduate students from various disciplines and backgrounds.
\end{itemize}
% \section*{Presentations}
\paragraph{Presentations}
\begin{itemize}\itemsep0em
\item Biostatistics department seminar (2022): \href{https://www.pubinfo.vcu.edu/calendar/details.asp?myVal=76554}{Virginia Commonwealth University}.
\item Statistics department seminar (2022): \href{https://statistics.wharton.upenn.edu/research/seminars-conferences/}{The Wharton School, University of Pennsylvania}.
\item Invited talk at \href{https://www.edas.info/ap/ciss2022/program.html}{56th Annual Conference on Information Sciences and Systems} (CISS 2022).
\item Statistics department seminar (2022): \href{https://www.isical.ac.in/~isru/index.html}{Interdisciplinary Statistical Research Unit, ISI, Kolkata}. 
\item Statistics department seminar (2022): \href{https://www.bristolmathsresearch.org/events/statistics/}{University of Bristol}.
\item Statistics department seminar (2021): \href{https://today.wisc.edu/events/view/164421}{University of Wisconsin–Madison}.
\item Biostatistics department seminar (2021): \href{https://publichealth.jhu.edu/departments/biostatistics/news-and-events/current-seminars}{Johns Hopkins University}. 
\item Statistics department seminar (2021): \href{https://mcgillstat.github.io/}{McGill University}.
\item Department seminar (2021): \href{https://www.iimb.ac.in/ds-webinar-series-2021-22}{Indian Institute of Management, Bangalore}.
\item Invited talk at \href{https://symposium2021.icsa.org/program/}{ICSA 2021: Applied Statistics Symposium}.
\item Invited talk at \href{https://www.isi2021.org/events/isi-world-statistics-congress-2021/index.html@session-type=ips-non-live&date=2021-07-16.html}{World Statistics Congress 2021}.
\item Invited talk at \href{https://www.intindstat.org/summerConference2021/scprogCommittee}{International Indian Statistical Association 2021}.
\item Invited talk at \href{https://www.selectiveinferenceseminar.com/}{International Seminar on Selective Inference 2020}.
\item Statistics department seminar (2020): \href{https://stat.uchicago.edu/events/event/1784/?past=y}{University of Chicago}.
\item Department seminar (2020): 
University of Chicago Booth School of Business.
\item Department seminar (2020): \href{https://www.hsph.harvard.edu/biostatistics/2020/02/upcoming-seminars/}{Harvard T.H. Chan School of Public Health}. \item Statistics department seminar (2020): \href{https://www.stat.ubc.ca/valid-post-selection-inference-why-and-how}{University of British Columbia}.
\item Statistics department seminar (2020): University of California, Irvine.
\item Statistics department seminar (2020): Rutgers University.
\item Invited talk at \href{https://2019mcp.smartevent.com.tw/}{MCP 2019}, National Taiwan University.
\item Invited talk at \href{https://www.math.wustl.edu/~kuffner/WHOA-PSI-4.html}{WHOA-PSI-4}, 2019.
\item Invited talk at \href{https://ww2.amstat.org/meetings/jsm/2019/onlineprogram/AbstractDetails.cfm?abstractid=302901}{JSM 2019}, Denver.
\item Invited talk at \href{http://www.icsa.org/icsa/events/2019-icsa-china-conference}{ICSA 2019}, Nankai University.
\item Invited talk at young researchers session, \href{https://cccfran.github.io/larrybrown2018/\#schedule}{Lawrence D Brown memorial workshop}.
\item Shared a talk with Andreas Buja at \href{https://www.math.wustl.edu/~kuffner/WHOA-PSI-3.html}{WHOA-PSI-3}, 2018.  
\item Invited talk at \href{https://www.univie.ac.at/seam/inference2018/}{Workshop Model Selection, Regularization, and Inference} 2018 (Represented Larry Brown).
\item Shared a talk with Andreas Buja at \href{https://publish.illinois.edu/sldsc2018/}{SLDSC 2018}.
\item Invited talk at \href{https://www.math.wustl.edu/~kuffner/WHOA-PSI-2.html}{WHOA-PSI-2}, 2017 (Represented Larry Brown).
\item Special topics session at \href{http://www.isi2015.ibge.gov.br/}{World Statistics Congress 2015}, Rio de Janeiro, Brazil.
\item Contributed session presentation at \href{https://www.isical.ac.in/~icors2015/}{ICORS 2015}, Kolkata, India.
\item Contributed session presentation at \href{https://statistik.wiwi.uni-halle.de/icors2014/}{ICORS 2014}, Halle, Germany.
\end{itemize}
\paragraph{Academic Achievements}
\begin{itemize}
\item NSF grant, \href{https://www.nsf.gov/awardsearch/showAward?AWD_ID=2113611&HistoricalAwards=false}{DMS-2113611} (2021--2024, \$300,000), ``Central Limit Theorems and Inference in High Dimensions.'' (Joint with Alessandro Rinaldo).
\item Student travel award, Wharton Doctoral Programs, George James Term Fund 2019.
\item Got second prize in \href{http://www.isi-web.org/index.php/activities/awards/isi-awards/tinbergen-award}{Jan Tinbergen} competition for young statisticians from developing countries.
\item Awarded \href{http://www.kvpy.iisc.ernet.in/main/fellows.htm}{Kishore Vaigyanik Protsahan Yojana} scholarship (2011--2015).
\item Inspire Scholar since April 2011. (From Department of Science and Technology, India)
% \item Selected in IIT-Joint Entrance Exam 2010 (All India Rank - 2469), AIEEE 2010 (AIR-2604), BITSAT 2010 (Score-343)
% \item Stood district second in mathematics competition conducted by Ramanujan Mathema-tics Academy
% in 2008.     
\end{itemize}
\paragraph{Committees}
\begin{itemize}\itemsep0em
    \item Served as one of the judges for International Indian Statistical Association (IISA) student paper competition for the Probability/Theory/Methodology section 2020.  
    \item Served on the Statistical Learning and Data Science (SLDS) 2021 Student Paper Award committee.
\end{itemize}
\paragraph{PhD students}
\begin{itemize}
    \item Xiaoyi Gu (co-advised with Alessandro Rinaldo). Currently at Amazon.
\end{itemize}
\paragraph{Journal Review}
Served as a reviewer for several statistical journals:
\begin{itemize}
    \item Annals of Statistics;
    \item Journal of American Statistical Association;
    \item Biometrika;
    \item Journal of the Royal Statistical Society: Series B (Statistical Methodology);
    \item Bernoulli;
    \item Information and Inference;
    \item Statistica Sinica;
    \item Electronic Journal of Statistics;
    \item Statistical Science;
    \item Statistical Papers;
    \item Statistics and Computing;
\end{itemize}
Also, served as a reviewer for machine learning conferences including COLT (conference on learning theory), UAI (Uncertainty in Artificial Intelligence). 
\paragraph{Programming and Scripting}
\begin{itemize}\itemsep0em
\item R -- Proficient,
\item C -- Basic Programming,
\item Python -- Basic Programming.
\end{itemize}
\end{document}